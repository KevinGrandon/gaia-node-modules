#! /usr/bin/env node
"use strict";

var _interopRequire = function (obj) {
  return obj && (obj["default"] || obj);
};

// This module is the entrypoint so it needs to import the polyfill...
require("6to5/polyfill");

var ArgumentParser = require("argparse").ArgumentParser;
var assert = _interopRequire(require("assert"));

var Debug = _interopRequire(require("debug"));

var npm = _interopRequire(require("../npm"));

var taskcluster = _interopRequire(require("taskcluster-client"));

var request = _interopRequire(require("superagent-promise"));

var fs = _interopRequire(require("mz/fs"));

var fsPath = _interopRequire(require("path"));

var eventToPromise = _interopRequire(require("event-to-promise"));

var hash = _interopRequire(require("../hash"));

var signature = _interopRequire(require("../signature"));

var debug = new Debug("npm-cache:put");
var parser = new ArgumentParser();
parser.addArgument(["--task-id"], {
  help: "The task to run caching logic for",
  required: true,
  dest: "taskId"
});

parser.addArgument(["--run-id"], {
  help: "Run id for the particular task",
  dest: "runId",
  defaultValue: 0
});

parser.addArgument(["--namespace"], {
  defaultValue: "npm_cache",
  help: "Index namespace to use"
});

parser.addArgument(["--proxy"], {
  action: "storeTrue",
  help: "Rely on the taskcluster proxy service provided by the docker-worker"
});

function upload(queue, taskId, runId, expires, modulePath) {
  var size, tar, artifact, artifactUrl, put;
  return regeneratorRuntime.async(function upload$(context$1$0) {
    while (1) switch (context$1$0.prev = context$1$0.next) {
      case 0:
        context$1$0.next = 2;
        return fs.stat(modulePath);
      case 2:
        size = context$1$0.sent.size;
        tar = fs.createReadStream(modulePath);
        artifact = {
          storageType: "s3",
          expires: expires,
          contentType: "application/x-tar"
        };
        context$1$0.next = 7;
        return queue.createArtifact(taskId, runId, "public/node_modules.tar.gz", artifact);
      case 7:
        artifactUrl = context$1$0.sent;
        put = request.put(artifactUrl.putUrl);
        put.set("Content-Length", size);
        put.set("Content-Type", "application/x-tar");
        put.set("Content-Encoding", "gzip");
        tar.pipe(put);
        put.end();
        context$1$0.next = 16;
        return eventToPromise(put, "end");
      case 16:
      case "end":
        return context$1$0.stop();
    }
  }, null, this);
}

function main() {
  var args, queueOpts, indexOpts, queue, index, task, url, expires, pkgReqs, pkg, pkgHash, namespace, indexedTask, workspace, moduleTar, indexPayload;
  return regeneratorRuntime.async(function main$(context$1$0) {
    while (1) switch (context$1$0.prev = context$1$0.next) {
      case 0:
        args = parser.parseArgs(process.argv.slice(2));
        queueOpts = {};
        indexOpts = {};


        if (args.proxy) {
          queueOpts.baseUrl = "taskcluster/queue/v1";
          indexOpts.baseUrl = "taskcluster/index/v1";
        }

        queue = new taskcluster.Queue(queueOpts);
        index = new taskcluster.Index(indexOpts);
        context$1$0.next = 8;
        return queue.getTask(args.taskId);
      case 8:
        task = context$1$0.sent;


        if (!task.extra || !task.extra.npmCache) {
          console.error("Task must contain task.extra");
          process.exit(1);
        }

        url = task.extra.npmCache.url;
        if (!url) {
          console.error("Task must contain a extra.npmCache.url");
          process.exit(1);
        }

        expires = new Date(task.extra.npmCache.expires);
        if (!task.extra.npmCache.expires || expires < new Date()) {
          console.error("Task must contain extra.npmCache.expires and be in the future.");
          process.exit(1);
        }

        context$1$0.next = 16;
        return request.get(url).end();
      case 16:
        pkgReqs = context$1$0.sent;
        pkg = JSON.parse(pkgReqs.text.trim());
        pkgHash = hash(pkgReqs.text.trim());
        namespace = "" + args.namespace + "." + signature() + "." + pkgHash;


        debug("Package hash =", pkgHash);
        debug("Package namespace =", namespace);

        context$1$0.prev = 22;
        context$1$0.next = 25;
        return index.findTask(namespace);
      case 25:
        indexedTask = context$1$0.sent;
        debug("Cache hit. Skipping tarball creation.");
        process.exit(0);
        context$1$0.next = 34;
        break;
      case 30:
        context$1$0.prev = 30;
        context$1$0.t1 = context$1$0["catch"](22);
        if (!(!err.statusCode || err.statusCode !== 404)) {
          context$1$0.next = 34;
          break;
        }
        throw context$1$0.t1;
      case 34:
        context$1$0.next = 36;
        return npm();
      case 36:
        workspace = context$1$0.sent;
        context$1$0.next = 39;
        return workspace.install(pkg);
      case 39:
        context$1$0.next = 41;
        return workspace.exportTar();
      case 41:
        moduleTar = context$1$0.sent;
        context$1$0.next = 44;
        return upload(queue, args.taskId, args.runId, expires, moduleTar);
      case 44:
        indexPayload = {
          taskId: args.taskId,
          rank: 0, // XXX: How should we define ranking?
          expires: expires,
          data: {}
        };
        context$1$0.next = 47;
        return index.insertTask(namespace, indexPayload);
      case 47:
      case "end":
        return context$1$0.stop();
    }
  }, null, this, [[22, 30]]);
}

main()["catch"](function (e) {
  process.nextTick(function () {
    console.error("Something is wrong...");
    throw e;
  });
});
// Check to see if we already have this package json cached...


// Insert the platform specific index namespace...
//# sourceMappingURL=taskcluster-npm-cache.js.map